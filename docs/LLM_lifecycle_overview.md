---

# ğŸŒ LLM Lifecycle Overview

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   1. Dataset      â”‚
        â”‚  (collect & prep) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   2. Pretraining  â”‚
        â”‚ (general language â”‚
        â”‚   knowledge)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   3. Fine-tuning  â”‚
        â”‚  (adapt to tasks, â”‚
        â”‚   style, domain)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   4. Inference    â”‚
        â”‚ (generation,      â”‚
        â”‚ decoding params,  â”‚
        â”‚   prompts)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“Š Phase by Phase

### **1. Dataset**

* **Goal:** Gather text data
* **Examples:** web text, Wikipedia, books, movie dialogues, Q\&A pairs
* **Your project:** Cornell Movie Dialogs â†’ `train.jsonl`, `val.jsonl`, `test.jsonl`

---

### **2. Pretraining**

* **Goal:** Teach base language & world knowledge
* **What you control:** Not usually (done by labs)
* **Output:** Base model (`microsoft/phi-3-mini-4k-instruct`)

---

### **3. Fine-Tuning**

* **Goal:** Specialize to task/style
* **Methods:** Full finetune, LoRA/QLoRA, Instruction Tuning, RLHF
* **Your setup:** LoRA on Cornell â†’ `out-cornell-phi3` + merged `out-cornell-phi3-merged`

---

### **4. Inference**

* **Goal:** Generate text in your app
* **Knobs:**

  * `temperature`, `top_p`, `top_k` â†’ creativity vs. focus
  * `max_new_tokens`, stopping criteria â†’ length control
  * `repetition_penalty`, `no_repeat_ngram_size` â†’ avoid loops
  * System message, chat template, history length â†’ tone & coherence
* **Your case:** interactive `inference.py` with conversation memory

---

ğŸ‘‰ The way to read this:

* **Dataset + Pretraining** = what the model *knows*
* **Fine-tuning** = how the model *behaves*
* **Inference** = how you *let it speak*

---

Would you like me to make this into a **polished diagram image** (boxes, arrows, color-coded) so you can drop it into your notes or slides?
